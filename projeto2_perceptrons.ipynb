{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projeto2_perceptrons.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phmouras/Critical-Phenomena-in-gravitational-collapse-via-spectral-methods/blob/main/projeto2_perceptrons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5vP_ogRktgm"
      },
      "source": [
        "# Projeto 2 - Perceptrons\n",
        "\n",
        "Nesse segundo projeto da disciplina de Introdução a Python, vamos construir uma classe de um modelo de Redes Neurais, que é a base das Redes Neurais clássicas e Redes Neurais Profundas (Deep Learning), chamado Peceptron.\n",
        "\n",
        "No mundo atual, o aprendizado de máquina está presente em muitas tecnologias à nossa volta: classificação de bilhões de imagens (por exemplo, Google Images), reconhecimento de fala (por exemplo, a Siri da Apple), recomendação de vídeos no YouTube, entre muitas outras.\n",
        "\n",
        "![Aprendizado Supervisionado](https://miro.medium.com/max/1400/1*eEKb2RxREV6-MtLz2DNWFQ.gif)\n",
        "\n",
        "O Perceptron é uma das arquiteturas de Redes Neurais Artificiais mais simples que existem. Foi inventado em 1957 por Frank Rosenblatt.\n",
        "\n",
        "Para entender como um Perceptron funciona, vamos começar com um esquema do perceptron original, sem muitos detalhes.\n",
        "\n",
        "O Perceptron:\n",
        "* toma alguns inputs,\n",
        "* faz um processamento\n",
        "* retorna um único output.\n",
        "\n",
        "![perceptrons](https://raw.githubusercontent.com/malbouis/Python_intro/master/aulas_2020-1/pics/perceptrons%201.png)\n",
        "\n",
        "Antes de continuar, vamos pensar em um exemplo concreto.\n",
        "\n",
        "Vamos considerar um conjunto de dados de flores Iris, e suas três espécies: Iris-Setosa, Iris-Versicolor e Iris-Virgina, que têm as seguintes características:\n",
        "* comprimento e largura da sépala;\n",
        "* comprimento e largura da pétala.\n",
        "\n",
        "**Iris dataset**: Famoso conjunto de dados com o comprimento e a largura das pétalas e sépalas de 150 flores Iris de 3 espécies diferentes: Iris-Setosa, Iris-Versicolor e Iris-Virginica.\n",
        "\n",
        "![](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)\n",
        "\n",
        "### Funcionamento do Perceptron\n",
        "\n",
        "Podemos pensar no Perceptron como um mecanismo que toma como input as *features* (características) de uma Iris (comprimento e largura da sépala e da pétala) e nos retorna a **classificação** da espécie da flor, ou a maior probabilidade de que seja de uma dada espécie.\n",
        "\n",
        "É importante notar que o Perceptron tem um comportamento **binário**, ou seja, só podemos comparar *duas* espécies de flores com o Perceptron.\n",
        "\n",
        "Na nossa representação simples do perceptron acima, teríamos:\n",
        "\n",
        "![](https://raw.githubusercontent.com/malbouis/Python_intro/master/aulas_2020-1/pics/perceptrons2.png)\n",
        "\n",
        "#### Processamento\n",
        "\n",
        "O Processamento do Perceptron, é onde se dá a decisão do mecanismo.\n",
        "\n",
        "A fórmula para a decisão do neurônio perceptron é relativamente simples:\n",
        "\n",
        "$\n",
        "    output=\\left\\{\n",
        "                \\begin{array}{ll}\n",
        "                  0, se \\sum_j w_j x_j \\leq b\\\\\n",
        "                  1, se \\sum_j w_j x_j > b\\\\\n",
        "                \\end{array}\n",
        "              \\right.\n",
        "  $\n",
        "\n",
        "Essa fórmula é conhecida como *Heaviside stepfunction*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "9ymCQOFAxHaf",
        "outputId": "814e6093-35aa-434f-c9d2-3cc29b2873f0"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace (-1.5,1.5)\n",
        "y = np.heaviside(x,0)\n",
        "\n",
        "plt.plot(x,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1daa333710>]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EV3Wrapyk2t"
      },
      "source": [
        "A fórmula de decisão do neurônio (perceptron), também é chamada de **função de ativação**.\n",
        "\n",
        "Podemos reescrevê-la da seguinte forma:\n",
        "\n",
        "$\n",
        "    ativação=\\left\\{\n",
        "                \\begin{array}{ll}\n",
        "                  1, se \\sum_j w_j x_j + b > 0\\\\\n",
        "                  0, se \\sum_j w_j x_j + b \\leq 0\\\\\n",
        "                \\end{array}\n",
        "              \\right.\n",
        "  $\n",
        "\n",
        "Podemos incorporar b no somatório da primeira linha da equação acima, considerando: $w_0=b$ e $x_0 = 1$. Sendo assim, a função de ativação pode ser reescrita como:\n",
        "\n",
        "$\n",
        "    f(x)=\\left\\{\n",
        "                \\begin{array}{ll}\n",
        "                  1, se \\sum_{j=0}^{N} w_j x_j > 0\\\\\n",
        "                  0, para \\; o \\; resto\\\\\n",
        "                \\end{array}\n",
        "              \\right.\n",
        "  $\n",
        "\n",
        "E assim incorporamos o $b$, que também é conhecido como **bias**, como  peso $w_0 = b$ do input $x_0=1$ do nosso peceptron, que vai tomando corpo:\n",
        "\n",
        "![](https://raw.githubusercontent.com/malbouis/Python_intro/master/aulas_2020-1/pics/perceptrons4.png)\n",
        "\n",
        "\n",
        "Por simplicidade, usaremos a função de ativação de Heaviside, entretanto essa função pode variar. Pode ser, por exemplo, uma função [*sigmoide*](https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_sigmoide)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F10ZKPulxGNt"
      },
      "source": [
        "## Como se dá a classificação das Iris? Ou, como os perceptrons aprendem?\n",
        "\n",
        "\n",
        "O primeiro passo é **treinar** o nosso Perceptron: em um conjunto de dados em que **sabemos as espécies das Iris a priori**;\n",
        "\n",
        "### Treino\n",
        "\n",
        "Para cada **conjunto de inputs** (features):\n",
        "   1. Compararamos o output ($f(x)$, a classificação do nosso perceptron, que chamaremos de **prediction**) com a espécie real da flor (chamaremos de y e identificaremos como **label**);\n",
        "   1. Baseados no **resultado da comparação entre prediction e label**, devemos **ajustar os pesos** caso o perceptron não esteja acertando corretamente a classificação.\n",
        "   1. O último passo é **testar** o nosso perceptron: em um outro conjunto de dados, comparamos a classificação fornecida pelo perceptron com a real espécie da flor (**label**);\n",
        "\n",
        "### Prediction\n",
        "\n",
        "A predição do perceptron é calculada pela função de ativação. No nosso exemplo, ela é a *Heaviside Stepfunction*, que escrevemos acima:\n",
        "\n",
        "$\n",
        "    f(x)=\\left\\{\n",
        "                \\begin{array}{ll}\n",
        "                  1, se \\sum_{j=0}^{N} w_j x_j > 0\\\\\n",
        "                  0, para \\; o \\; resto\\\\\n",
        "                \\end{array}\n",
        "              \\right.\n",
        "  $\n",
        "\n",
        "* Note que tanto os inputs quanto os pesos são matrizes (ou arrays), logo o produto do somatório é um produto matricial (ou escalar). *Dica: o numpy tem uma função [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) que implementa produto escalar.*\n",
        "\n",
        "* Note também que na nossa construção do perceptron, o primeiro elemento do array dos inputs é o número 1 e o primeiro elemento do array dos pesos é o b (**bias**).\n",
        "\n",
        "### Comparação entre prediction / label e ajuste do perceptron\n",
        "\n",
        "Consideremos que o nosso perceptron teve um output, $f(x)$ e que o output esperado (o real, do qual nós temos a informação a priori, o label) é $y$.\n",
        "\n",
        "A comparação entre o output esperado e o obtido é simplesmente:\n",
        "\n",
        "`e = output_esperado - output_obtido`\n",
        "\n",
        "ou\n",
        "\n",
        "$e = y - f(x)$\n",
        "\n",
        "O que queremos é rodar o perceptron sobre os inputs até que a diferença acima (ou o erro na predição) seja o menor possível.\n",
        "\n",
        "A melhor forma de fazer isso é corrigindo os pesos de cada input, pelo erro de predição.\n",
        "\n",
        "### Ajuste dos pesos\n",
        "\n",
        "O ajuste dos pesos é dado por:\n",
        "\n",
        "$w \\rightarrow w + lr*e*x$\n",
        "\n",
        "onde\n",
        "\n",
        "`lr`: learning rate\n",
        "\n",
        "`e`: erro do output em relação ao label\n",
        "\n",
        "`x`: conjunto de inputs (features)\n",
        "\n",
        "\n",
        "Essa atualização dos pesos é feita de forma iterativa e o número de iterações é determinado pelo que chamamos de **threshold**, também conhecido como **época**.\n",
        "\n",
        "\n",
        "## PROJETO\n",
        "\n",
        "1. Criar uma `Classe` Perceptron em Python. Vocês podem seguir o esqueleto de Classe apresentado abaixo.\n",
        "\n",
        "2. Aplique essa classe nos dados das flores Iris para determinar se uma Iris com um certo comprimento e largura da **pétala** é uma Iris Setosa.\n",
        "\n",
        "3. Compare o resultado da sua classe com a classe Peceptron do módulo do scikit-learn `linear_model`.\n",
        "\n",
        "4. Você pode pensar em algum outro exemplo em que possa aplicar o modelo do Perceptron? Quando esse modelo falha?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JDBy4GmuHeT"
      },
      "source": [
        "## ESQUELETO DE CLASSE PERCEPTRON\n",
        "import numpy as np\n",
        "\n",
        "def train_test_split(X, y, test_size = 0.2):    # X = features, y = label\n",
        "  l = int(len(X)*(1-test_size))\n",
        "  return X[:l], X[l:], y[:l], y[l:]             # [:l] = take de l first elements of the list . Comparison the prediction with labels\n",
        "\n",
        "class MyPerceptron():\n",
        "\n",
        "    def __init__(self, no_of_inputs, threshold=100, learning_rate=0.01):\n",
        "        '''\n",
        "        método de inicialização que tem os seguintes atributos:\n",
        "        no_of_inputs: número de features passadas como input ao perceptron\n",
        "        threshold: número de iterações de atualização do peso\n",
        "        learning_rate: taxa com a qual os pesos são atualizados a cada iteração\n",
        "        weights: inicialização dos pesos (dica: pode inicializar com método np.zeros). Não se esquecer que o vetor dos pesos\n",
        "        terá no_of_inputs + 1 elementos por conta do bias que é o primeiro elemento.\n",
        "        '''\n",
        "        self.no_of_inputs = no_of_inputs\n",
        "        self.threshold = threshold\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = np.zeros(no_of_inputs+1)\n",
        "\n",
        "    def predict(self, inputs):\n",
        "      '''\n",
        "      método de implementação da função de ativação.\n",
        "      inputs: array com o conjunto de inputs (features). No projeto pedimos que considerassem o comprimento e largura da pétala da Iris.\n",
        "      Não se esquecer que o produto da função de ativação é um produto escalar e pode ser calculado pelo método np.dot\n",
        "      '''\n",
        "      inputs = np.insert(inputs,0,1,axis=1)\n",
        "      output_get = []\n",
        "      for i in range(len(inputs)):\n",
        "        neuron = np.dot(inputs[i],self.weights)      # ativation function (Heaviside)\n",
        "        if neuron > 0:\n",
        "          output_get.append(1)\n",
        "        else:\n",
        "          output_get.append(0)\n",
        "      return output_get\n",
        "\n",
        "    def train(self, training_inputs, labels):\n",
        "      '''\n",
        "      método de treino. É aqui que os pesos são atualizados um certo número de vezes (determinado pelo valor do threshold).\n",
        "      Nesse método é feita a comparação entre o resultado da função de ativação (predição) e\n",
        "      o resultado esperado (label).\n",
        "      O método deve atualizar tanto os pesos quanto o bias (lembre que o bias é o primeiro valor do vetor peso e tem input 1.)\n",
        "      A atualização é feita iterativamente um número (threshold) de vezes.\n",
        "      '''\n",
        "      for t in range(self.threshold):\n",
        "        output_get = self.predict(training_inputs)\n",
        "        erro = labels - output_get\n",
        "        add_weights = np.sum(self.learning_rate*(erro.reshape(len(erro),1)*np.insert(training_inputs,0,1,axis=1)),axis=0)\n",
        "        self.weights += add_weights\n",
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrqxx1RXgRlH",
        "outputId": "a0bd97fd-fb20-432f-ed1a-accb8b75e3ed"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "iris = load_iris() # returns a dictionary-like object\n",
        "# características (features) das flores Iris:\n",
        "#print(iris.target_names)    # target = label\n",
        "#print(iris.feature_names)\n",
        "\n",
        "# Pelo primeiro print acima, as features são: 0: sepal length, 1: sepal width, 2: petal length, 3: petal width\n",
        "# Vamos selecionar somente as features petal length e petal width:\n",
        "X = iris.data[:,:]  # nos retorna um array com 150 conjuntos de inputs\n",
        "#len(iris.data[:,(2,3)])\n",
        "y = (iris.target == 0).astype(int)  # label = iris setosa. Retorna uma lista com os labels da Iris-Setosa. 0: não, 1: sim\n",
        "#print(\"iris setosa classification: \", y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "perceptron_clf = Perceptron() # dois inputs: comprimento e largura da pétala\n",
        "perceptron_clf.fit(X_train,y_train)  # Train\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "score_train = r2_score(y_train,perceptron_clf.predict(X_train))\n",
        "print(f'R²:{score_train} (train)')\n",
        "\n",
        "score_test = r2_score(y_test,perceptron_clf.predict(X_test))\n",
        "print(f'R²:{score_test} (test)')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R²:1.0 (train)\n",
            "R²:1.0 (test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PFjpBI4YAror"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5UxGH3PS7HH",
        "outputId": "b002fdf6-d0ba-4ff8-8b1a-9f130418e54f"
      },
      "source": [
        "# Modelo de como deve funcionar com a classe criada nesse projeto\n",
        "\n",
        "X = iris.data[:,:] # inputs\n",
        "y = (iris.target == 0).astype(int) # label = iris setosa. Retorna uma lista com os labels da Iris-Setosa. 0: não, 1: sim\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "perceptron = MyPerceptron(np.shape(X)[1])\n",
        "perceptron.train(X_train,y_train)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "score_train = r2_score(y_train,perceptron.predict(X_train))\n",
        "print(f'R²:{score_train} (train)')\n",
        "\n",
        "score_test = r2_score(y_test,perceptron.predict(X_test))\n",
        "print(f'R²:{score_test} (test)')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R²:1.0 (train)\n",
            "R²:1.0 (test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cMvfMv-5Bt0P"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nwzsUMmuPTW"
      },
      "source": [
        "3. Os resultados obtidos usando o sklearn e o nosso modelo são identicos\n",
        "4. Poderiamos aplicar o modelo na classificação de caracteristicas que definem um cachorro, tendo como resultado se é ou não é um cachorro. Basicamente ele funciona somente em classificação binaria (sé ou não é). Não funcionaria nesse mesmo exemplo se tivessemos que classificar os 3 tipos de iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwHa1JfYPnag"
      },
      "source": [
        "No exemplo acima, usamos somente dois inputs: o comprimento e a largura da pétala.\n",
        "* Você pode expandir o código para considerar também o comprimento e largura da sépala como features;\n",
        "* Você pode separar a amostra das Iris em duas: uma para treino e outra para teste. O scikit-learn tem um método que faz isso pra você: o `train_test_slit`; Compare cada uma das Iris de teste com o label ==> coloque o resultado em um gráfico (classificação versus label);\n",
        "* **Ponto extra** para quem implementar a função de ativação sigmoid no lugar da Heaviside e/ou melhorar a estimativa do erro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZxB90UhrA7U"
      },
      "source": [
        "## ESQUELETO DE CLASSE PERCEPTRON\n",
        "import numpy as np\n",
        "from scipy.special import expit       # sigmoid function import\n",
        "\n",
        "\n",
        "def train_test_split(X, y, test_size = 0.2):\n",
        "  l = int(len(X)*(1-test_size))\n",
        "  return X[:l], X[l:], y[:l], y[l:]\n",
        "\n",
        "class MyPerceptron():\n",
        "\n",
        "    def __init__(self, no_of_inputs, threshold=100, learning_rate=0.01):\n",
        "        '''\n",
        "        método de inicialização que tem os seguintes atributos:\n",
        "        no_of_inputs: número de features passadas como input ao perceptron\n",
        "        threshold: número de iterações de atualização do peso\n",
        "        learning_rate: taxa com a qual os pesos são atualizados a cada iteração\n",
        "        weights: inicialização dos pesos (dica: pode inicializar com método np.zeros). Não se esquecer que o vetor dos pesos\n",
        "        terá no_of_inputs + 1 elementos por conta do bias que é o primeiro elemento.\n",
        "        '''\n",
        "        self.no_of_inputs = no_of_inputs\n",
        "        self.threshold = threshold\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = np.zeros(no_of_inputs+1)\n",
        "\n",
        "    def predict(self, inputs, func):\n",
        "      '''\n",
        "      método de implementação da função de ativação.\n",
        "      inputs: array com o conjunto de inputs (features). No projeto pedimos que considerassem o comprimento e largura da pétala da Iris.\n",
        "      Não se esquecer que o produto da função de ativação é um produto escalar e pode ser calculado pelo método np.dot\n",
        "      '''\n",
        "      inputs = np.insert(inputs,0,1,axis=1)\n",
        "      output_get = []\n",
        "      for i in range(len(inputs)):\n",
        "        z = np.dot(inputs[i],self.weights)\n",
        "        neuron = func(z)      #sigmoid\n",
        "        if neuron > 0:\n",
        "          output_get.append(1)\n",
        "        else:\n",
        "          output_get.append(0)\n",
        "      return output_get\n",
        "\n",
        "    def train(self, training_inputs, labels):\n",
        "      '''\n",
        "      método de treino. É aqui que os pesos são atualizados um certo número de vezes (determinado pelo valor do threshold).\n",
        "      Nesse método é feita a comparação entre o resultado da função de ativação (predição) e\n",
        "      o resultado esperado (label).\n",
        "      O método deve atualizar tanto os pesos quanto o bias (lembre que o bias é o primeiro valor do vetor peso e tem input 1.)\n",
        "      A atualização é feita iterativamente um número (threshold) de vezes.\n",
        "      '''\n",
        "      for t in range(self.threshold):\n",
        "        output_get = self.predict(training_inputs, expit)\n",
        "        erro = labels - output_get\n",
        "        add_weights = np.sum(self.learning_rate*(erro.reshape(len(erro),1)*np.insert(training_inputs,0,1,axis=1)),axis=0)\n",
        "        self.weights += add_weights\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12NvcDOB-HRx",
        "outputId": "16da3dbc-dbaa-44f3-8f71-d15b0efbb1ac"
      },
      "source": [
        "# Modelo de como deve funcionar com a classe criada nesse projeto\n",
        "\n",
        "X = iris.data[:,:] # inputs\n",
        "y = (iris.target == 0).astype(int) # label = iris setosa. Retorna uma lista com os labels da Iris-Setosa. 0: não, 1: sim\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "perceptron = MyPerceptron(np.shape(X)[1])\n",
        "perceptron.train(X_train,y_train)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "score_train = r2_score(y_train,perceptron.predict(X_train, expit))\n",
        "print(f'R²:{score_train} (train)')\n",
        "\n",
        "score_test = r2_score(y_test,perceptron.predict(X_test, expit))\n",
        "print(f'R²:{score_test} (test)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R²:0.8971428571428571 (train)\n",
            "R²:1.0 (test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCZCUOXE-eeI"
      },
      "source": [
        "Utilizando o sigmoid, o resultado do nosso modelo se mostrou eficiente, próximo ao resultado do sklearn. Além de apresentar um treino menor que 1, o que pode indicar que não tivemos overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRa5a1oDlP6o"
      },
      "source": [
        "## Referências:\n",
        "\n",
        "1. Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow, Aurélion Géron, O'Reilly;\n",
        "1. *Introdução ao numpy*: https://sebastianraschka.com/pdf/books/dlb/appendix_f_numpy-intro.pdf"
      ]
    }
  ]
}